{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f967bbc",
   "metadata": {},
   "source": [
    "# Genomics & Variant Effect Prediction â€” Starter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828b873b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve, roc_curve, confusion_matrix, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import dump\n",
    "\n",
    "# Point to your CSV (demo or ClinVar-cleaned)\n",
    "DATA_PATH = Path(\"../data/demo_variants.csv\")  # replace with ../data/clinvar_clean.csv when ready\n",
    "LABEL_COL = \"clinical_significance\"\n",
    "POS_LABEL = \"Pathogenic\"\n",
    "NEG_LABEL = \"Benign\"\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# If the label column has a different name, try to normalize it\n",
    "if LABEL_COL not in df.columns:\n",
    "    for c in [\"ClinicalSignificance\",\"clinical_significance_norm\",\"clinicalsignificance\",\"clinsig\",\"CLNSIG\"]:\n",
    "        if c in df.columns:\n",
    "            df = df.rename(columns={c: LABEL_COL})\n",
    "            break\n",
    "\n",
    "# Normalize label values if needed\n",
    "def _norm(s):\n",
    "    s = str(s).lower().replace(\" \", \"_\")\n",
    "    if \"pathogenic\" in s:   # includes likely_pathogenic\n",
    "        return \"Pathogenic\"\n",
    "    if \"benign\" in s:       # includes likely_benign\n",
    "        return \"Benign\"\n",
    "    return None\n",
    "\n",
    "df[LABEL_COL] = df[LABEL_COL].map(_norm)\n",
    "df = df[df[LABEL_COL].isin([POS_LABEL, NEG_LABEL])].copy()\n",
    "\n",
    "print(\"Head:\\n\", df.head(), \"\\n\")\n",
    "print(\"Label counts:\\n\", df[LABEL_COL].value_counts(), \"\\n\")\n",
    "\n",
    "# Ensure required numeric features exist\n",
    "for col in [\"grantham\",\"is_conserved\",\"in_domain\",\"hydrophobicity_delta\",\"charge_delta\"]:\n",
    "    if col not in df.columns:\n",
    "        df[col] = 0.0\n",
    "\n",
    "X = df[[\"grantham\",\"is_conserved\",\"in_domain\",\"hydrophobicity_delta\",\"charge_delta\"]].copy()\n",
    "y = (df[LABEL_COL] == POS_LABEL).astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "print(\"Shapes:\", X_train.shape, X_test.shape, \" PosRate(test)=\", y_test.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8669f1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build models\n",
    "logit = Pipeline([(\"scaler\", StandardScaler()),\n",
    "                  (\"clf\", LogisticRegression(max_iter=200, class_weight=\"balanced\"))])\n",
    "\n",
    "rf = Pipeline([(\"clf\", RandomForestClassifier(n_estimators=300, class_weight=\"balanced\", random_state=42))])\n",
    "\n",
    "models = {\"LogisticRegression\": logit, \"RandomForest\": rf}\n",
    "metrics = {}\n",
    "for name, m in models.items():\n",
    "    m.fit(X_train, y_train)\n",
    "    proba = m.predict_proba(X_test)[:,1]\n",
    "    metrics[name] = {\n",
    "        \"roc_auc\": roc_auc_score(y_test, proba),\n",
    "        \"pr_auc\": average_precision_score(y_test, proba)\n",
    "    }\n",
    "metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec32cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot ROC and PR for each model (matplotlib only, one plot per figure, no explicit colors)\n",
    "def plot_roc(y_true, y_score, title):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label=\"ROC\")\n",
    "    plt.plot([0,1],[0,1],\"--\")\n",
    "    plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\"); plt.title(title); plt.legend(); plt.show()\n",
    "\n",
    "def plot_pr(y_true, y_score, title):\n",
    "    p, r, _ = precision_recall_curve(y_true, y_score)\n",
    "    plt.figure()\n",
    "    plt.plot(r, p, label=\"PR\")\n",
    "    plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(title); plt.legend(); plt.show()\n",
    "\n",
    "for name, m in models.items():\n",
    "    proba = m.predict_proba(X_test)[:,1]\n",
    "    plot_roc(y_test, proba, f\"{name} ROC\")\n",
    "    plot_pr(y_test, proba, f\"{name} PR\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbba5c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Confusion matrices and classification reports @ 0.5\n",
    "for name, m in models.items():\n",
    "    proba = m.predict_proba(X_test)[:,1]\n",
    "    pred = (proba >= 0.5).astype(int)\n",
    "    print(f\"\\n{name} Confusion Matrix:\\n\", confusion_matrix(y_test, pred))\n",
    "    print(classification_report(y_test, pred, target_names=[NEG_LABEL, POS_LABEL]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adada9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Feature importances / coefficients\n",
    "feat_names = X.columns.tolist()\n",
    "\n",
    "# Logistic Regression coefficients (after scaling)\n",
    "m = models[\"LogisticRegression\"].fit(X_train, y_train)\n",
    "lr = m.named_steps[\"clf\"]\n",
    "import pandas as pd\n",
    "lr_coef = pd.DataFrame({\"feature\": feat_names, \"coef\": lr.coef_[0]}).sort_values(\"coef\", ascending=False)\n",
    "print(\"\\nLogistic Regression coefficients:\\n\", lr_coef)\n",
    "\n",
    "# RandomForest importances\n",
    "m = models[\"RandomForest\"].fit(X_train, y_train)\n",
    "rf = m.named_steps[\"clf\"]\n",
    "rf_imp = pd.DataFrame({\"feature\": feat_names, \"importance\": rf.feature_importances_}).sort_values(\"importance\", ascending=False)\n",
    "print(\"\\nRandomForest importances:\\n\", rf_imp)\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(rf_imp[\"feature\"], rf_imp[\"importance\"])\n",
    "plt.title(\"RandomForest Feature Importances\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc5abe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save best model by PR-AUC\n",
    "best_name = max(metrics, key=lambda k: metrics[k][\"pr_auc\"])\n",
    "best_model = models[best_name].fit(X_train, y_train)\n",
    "MODEL_PATH = Path(f\"../model_{best_name.lower()}.joblib\")\n",
    "dump(best_model, MODEL_PATH)\n",
    "print(\"Saved best model:\", best_name, \"->\", MODEL_PATH.resolve())\n",
    "\n",
    "def score_variant(model, features: dict) -> float:\n",
    "    row = pd.DataFrame([features])[[\"grantham\",\"is_conserved\",\"in_domain\",\"hydrophobicity_delta\",\"charge_delta\"]]\n",
    "    return float(model.predict_proba(row)[:,1][0])\n",
    "\n",
    "print(\"Example score:\", score_variant(best_model, {\n",
    "    \"grantham\": 120,\n",
    "    \"is_conserved\": 1,\n",
    "    \"in_domain\": 1,\n",
    "    \"hydrophobicity_delta\": 0.2,\n",
    "    \"charge_delta\": 1\n",
    "}))\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
